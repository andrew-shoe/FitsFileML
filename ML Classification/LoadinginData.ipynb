{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Dataframes\n",
    "\n",
    "- **corr_pair.pkl**: dataframe of the correlation information of all ON/OFF pairs.\n",
    "- **corr_dataframe.pkl**: corr_pair.pkl dataframe, with added ON and OFF target coordinates.\n",
    "- **comb_corr_hog.pkl**: dataframe of the correlation + HOG (unnormalized) information of ON/OFF pairs.\n",
    "- **mod_corr_dataframe.pkl**: dataframe of header information and correlation information of all ON/OFF pairs.\n",
    "- **dataframe.pkl**: dataframes 'comb_corr_hog.pkl' and 'mod_corr_dataframe.pkl' combined\n",
    "\n",
    "- **binary_dataframe.pkl**: dataframe of the correlation + HOG (binarized) information of ON/OFF pairs.\n",
    "- **log_dataframe.pkl**: dataframe of the correlation + HOG (log normalized) information.\n",
    "- **binary_hog_dataframe.pkl**: dataframe of correlation (only aligned) + HOG (unnormalized) + HOG (binarized) information of ON/OFF pairs.\n",
    "- **binary_log_dataframe.pkl**: dataframe of correlation (only aligned) + HOG (log normalized) + HOG (binarized) information of ON/OFF pairs.\n",
    "- **log_hog_dataframe.pkl**: dataframe of correlation (only aligned) + HOG (log normalized) + HOG (unnormalized) information of ON/OFF pairs.\n",
    "- **binary_log_hog_dataframe.pkl**: dataframe of correlation (only aligned) + HOG (binarized) + HOG (log normalized) + HOG (unnormalized) information of ON/OFF pairs.\n",
    "- **labelled_binary_log_hog_dataframe.pkl**: dataframe of correlation (only aligned) + HOG (binarized) + HOG (log normalized) + HOG (unnormalized) + labels (see Probabilities CSV Jupyter notebook for what the numbers mean) information of ON/OFF pairs. \n",
    "\n",
    "\n",
    "## Arrays\n",
    "\n",
    "- **pairs.npy** contains the correlation information of all the pairs\n",
    "- **header.npy** contains the header information of all the pairs\n",
    "    - shape[0]=2, the 0th item is the ON header information; the 1st item is the OFF header information\n",
    "    - shape[x][0]=4\n",
    "        - FCNTR\n",
    "        - RA\n",
    "        - DEC\n",
    "        - MJD\n",
    "\n",
    "\n",
    "- **labelled_files.npy** contains the labels of all the labelled files. Read the Jupyter notebook 'Probabilities CSV' to see how labels were loaded in.\n",
    "\n",
    "**For everything below, see function hog_fd() to see how bins were shifted**\n",
    "\n",
    "- **hog_image_64by16_72_wb_ed.npy** contains the flattened vectors of the HOG transformations applied to each of the ON/OFF pairs (grey-scale). It is a list of list of lists: \n",
    "    - [[HOG information in 576 buckets for ON file #1, HOG information in 576 buckets for OFF file #1], ..., [HOG information in 576 buckets for ON file #47241, HOG information in 576 buckets for OFF file #47241]]\n",
    "    \n",
    "    \n",
    "- **hog_image_64by16_72_wb_ed_p.npy** contains the flattened vectors of 'hog_image_64by16_72_wb.npy' but with adjusted bins: we only collapsed (summed up) the diagonals; left the positional information for the straight(ish) lines. It is a list of list of lists:\n",
    "    - [[HOG information in 21 buckets for ON file #1, HOG information in 21 buckets for OFF file #1], ..., [HOG information in 21 buckets for ON file #47241, HOG information in 21 buckets for OFF file #47241]]\n",
    "\n",
    "\n",
    "- **binary_hog.npy** contains the flattened vectors of the HOG transformations applied to each of the ON/OFF pairs (grey-scale and binarized). It is a list of list of lists:\n",
    "    - [[binarized HOG information in 576 buckets for ON file #1, binarized HOG information in 576 buckets for OFF file #1], ..., [binarized HOG information in 576 buckets for ON file #47241, binarized HOG information in 576 buckets for OFF file #47241]]\n",
    "    \n",
    "    \n",
    "- **binary_hog_p.npy** contains the flattened vectors of 'binary_hog.npy' but with adjusted bins: we only collapsed (summed up) the diagonals; left the positional information for the straight(ish) lines. It is a list of list of lists:\n",
    "    - [[binarized HOG information in 21 buckets for ON file #1, binarized HOG information in 21 buckets for OFF file #1], ..., [binarized HOG information in 21 buckets for ON file #47241, binarized HOG information in 21 buckets for OFF file #47241]]\n",
    "\n",
    "\n",
    "- **hog_image_64by16_72_log.npy** contains the flattened vectors of the HOG transformations applied to each of the ON/OFF pairs (grey-scale and log normalized). It is a list of list of lists:\n",
    "    - [[log normalized HOG information in 576 buckets for ON file #1, log normalized HOG information in 576 buckets for OFF file #1], ..., [log normalized HOG information in 576 buckets for ON file #47241, log normalized HOG information in 576 buckets for OFF file #47241]]\n",
    "\n",
    "\n",
    "- **hog_image_64by16_72_log_p.npy** contains the flattened vectors of 'hog_image_64by16_72_log' but with adjusted bins: we only collapsed (summed up) the diagonals; left the positional information for the straight(ish) lines. It is a list of list of lists:\n",
    "    - [[log normalized HOG information in 21 buckets for ON file #1, log normalized HOG information in 21 buckets for OFF file #1], ..., [log normalized HOG information in 21 buckets for ON file #47241, log normalized HOG information in 21 buckets for OFF file #47241]]\n",
    "    \n",
    "\n",
    "**The below files were only used in the Positive Naive Bayes Classifier:**\n",
    "- **targ0_no_OFF.npy** contains names of targ0 stars w/ GPS signal in ON, but w/o signal in OFF (SOI)\n",
    "- **targ1_no_OFF.npy** contains names of targ1 stars w/ GPS signal in ON, but w/o signal in OFF (SOI)\n",
    "- **targ2_no_OFF.npy** contains names of targ2 stars w/ GPS signal in ON, but w/o signal in OFF (SOI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fitsio\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fits_corr import *\n",
    "from fits_corr.libs.utils import *\n",
    "\n",
    "import skimage\n",
    "from skimage.feature import hog, canny\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from astropy.io import fits\n",
    "import fitsio as fio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in / locating FITS pairs information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = ['0004', '0006', '0008', '0010', '0012', '0014', '0016', '0018', '0020']\n",
    "\n",
    "NUM_PAIRS=47242\n",
    "\n",
    "pairs = []\n",
    "pairs_data = []\n",
    "\n",
    "hog_pairs = []\n",
    "\n",
    "result_array_ON = np.empty((0, 21))\n",
    "result_array_OFF = np.empty((0, 21))\n",
    "\n",
    "fits_data_loc = \"/Users/mulan/desktop/fits_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n",
      "Collecting Pairs:  100% [-] Time: 0:00:00                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 47242 pairs to examine.\n"
     ]
    }
   ],
   "source": [
    "for s in folders:\n",
    "    pairs.extend(filter(lambda (a,b): fitsio.read(a).shape[0] == 16, get_on_off_pairs('./fits_data/' + s)))\n",
    "    \n",
    "print(\"We have \" + str(len(pairs)) + \" pairs to examine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hog_fd() function: condenses 576 HOG values into 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hog_fd(hog_fd3):\n",
    "    \n",
    "    \"\"\" Our input is an array of HOG information of\n",
    "        ON/OFF pairs. Each have length 576 of HOG\n",
    "        information. Note that since we had 8 windows,\n",
    "        when using HOG, every 72 values correspond\n",
    "        to a window. We want to condense these 576\n",
    "        into 21 bins. We want to preserve the vertical\n",
    "        and horizontal lines (so we preserve positioning\n",
    "        of the first and last windows), and then condense\n",
    "        the positioning of information diagonal lines. \"\"\"\n",
    "    \n",
    "    global result_array_ON\n",
    "    global result_array_OFF\n",
    "    \n",
    "    for i in range(NUM_PAIRS): \n",
    "        a = []\n",
    "        b = []\n",
    "        a.extend((hog_fd3[i][0][0], hog_fd3[i][0][72], hog_fd3[i][0][144], hog_fd3[i][0][216], \n",
    "                           hog_fd3[i][0][288], hog_fd3[i][0][360], hog_fd3[i][0][432], hog_fd3[i][0][504])) \n",
    "    \n",
    "        b.extend((hog_fd3[i][1][0], hog_fd3[i][1][72], hog_fd3[i][1][144], hog_fd3[i][1][216], \n",
    "                           hog_fd3[i][1][288], hog_fd3[i][1][360], hog_fd3[i][1][432], hog_fd3[i][1][504]))\n",
    "        \n",
    "        a.append(np.array([hog_fd3[i][0][1:18].sum(), hog_fd3[i][0][73:90].sum(), hog_fd3[i][0][145:162].sum(),\n",
    "                           hog_fd3[i][0][217:234].sum(), hog_fd3[i][0][289:306].sum(), hog_fd3[i][0][361:378].sum(),\n",
    "                           hog_fd3[i][0][433:450].sum(), hog_fd3[i][0][505:522].sum()]).sum())\n",
    "        \n",
    "        b.append(np.array([hog_fd3[i][1][1:18].sum(), hog_fd3[i][1][73:90].sum(), hog_fd3[i][1][145:162].sum(),\n",
    "                           hog_fd3[i][1][217:234].sum(), hog_fd3[i][1][289:306].sum(), hog_fd3[i][1][361:378].sum(),\n",
    "                           hog_fd3[i][1][433:450].sum(), hog_fd3[i][1][505:522].sum()]).sum())\n",
    "                \n",
    "        a.append(np.array([hog_fd3[i][0][18:35].sum(), hog_fd3[i][0][90:107].sum(), hog_fd3[i][0][162:179].sum(),\n",
    "                           hog_fd3[i][0][234:251].sum(), hog_fd3[i][0][306:323].sum(), hog_fd3[i][0][378:395].sum(),\n",
    "                           hog_fd3[i][0][450:467].sum(), hog_fd3[i][0][522:539].sum()]).sum())\n",
    "        \n",
    "        b.append(np.array([hog_fd3[i][1][18:35].sum(), hog_fd3[i][1][90:107].sum(), hog_fd3[i][1][162:179].sum(),\n",
    "                           hog_fd3[i][1][234:251].sum(), hog_fd3[i][1][306:323].sum(), hog_fd3[i][1][378:395].sum(),\n",
    "                           hog_fd3[i][1][450:467].sum(), hog_fd3[i][1][522:539].sum()]).sum())\n",
    "        \n",
    "        a.append(np.array([hog_fd3[i][0][35:37].sum(), hog_fd3[i][0][108:110].sum(), hog_fd3[i][0][179:181].sum(),\n",
    "                           hog_fd3[i][0][251:253].sum(), hog_fd3[i][0][323:325].sum(), hog_fd3[i][0][395:397].sum(),\n",
    "                           hog_fd3[i][0][467:469].sum(), hog_fd3[i][0][539:541].sum()]).sum())\n",
    "        \n",
    "        b.append(np.array([hog_fd3[i][1][35:37].sum(), hog_fd3[i][1][108:110].sum(), hog_fd3[i][1][179:181].sum(),\n",
    "                           hog_fd3[i][1][251:253].sum(), hog_fd3[i][1][323:325].sum(), hog_fd3[i][1][395:397].sum(),\n",
    "                           hog_fd3[i][1][467:469].sum(), hog_fd3[i][1][539:541].sum()]).sum())\n",
    "        \n",
    "        a.append(np.array([hog_fd3[i][0][37:54].sum(), hog_fd3[i][0][110:127].sum(), hog_fd3[i][0][181:198].sum(),\n",
    "                           hog_fd3[i][0][253:270].sum(), hog_fd3[i][0][325:342].sum(), hog_fd3[i][0][397:414].sum(),\n",
    "                           hog_fd3[i][0][469:486].sum(), hog_fd3[i][0][541:558].sum()]).sum())\n",
    "        \n",
    "        b.append(np.array([hog_fd3[i][1][37:54].sum(), hog_fd3[i][1][110:127].sum(), hog_fd3[i][1][181:198].sum(),\n",
    "                           hog_fd3[i][1][253:270].sum(), hog_fd3[i][1][325:342].sum(), hog_fd3[i][1][397:414].sum(),\n",
    "                           hog_fd3[i][1][469:486].sum(), hog_fd3[i][1][541:558].sum()]).sum())\n",
    "        \n",
    "        a.append(np.array([hog_fd3[i][0][54:71].sum(), hog_fd3[i][0][127:144].sum(), hog_fd3[i][0][198:215].sum(),\n",
    "                           hog_fd3[i][0][270:287].sum(), hog_fd3[i][0][342:359].sum(), hog_fd3[i][0][414:431].sum(),\n",
    "                           hog_fd3[i][0][486:503].sum(), hog_fd3[i][0][558:575].sum()]).sum())\n",
    "        \n",
    "        b.append(np.array([hog_fd3[i][1][54:71].sum(), hog_fd3[i][1][127:144].sum(), hog_fd3[i][1][198:215].sum(),\n",
    "                           hog_fd3[i][1][270:287].sum(), hog_fd3[i][1][342:359].sum(), hog_fd3[i][1][414:431].sum(),\n",
    "                           hog_fd3[i][1][486:503].sum(), hog_fd3[i][1][558:575].sum()]).sum())\n",
    "\n",
    "        a.extend((hog_fd3[i][0][71], hog_fd3[i][0][144], hog_fd3[i][0][215], hog_fd3[i][0][287], \n",
    "                           hog_fd3[i][0][359], hog_fd3[i][0][431], hog_fd3[i][0][503], hog_fd3[i][0][575])) \n",
    "        \n",
    "        b.extend((hog_fd3[i][1][71], hog_fd3[i][1][144], hog_fd3[i][1][215], hog_fd3[i][1][287], \n",
    "                           hog_fd3[i][1][359], hog_fd3[i][1][431], hog_fd3[i][1][503], hog_fd3[i][1][575])) \n",
    "\n",
    "        result_array_ON = np.append(result_array_ON, [a], axis=0)\n",
    "        result_array_OFF = np.append(result_array_OFF, [b], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pairs.npy: Loading in Correlation Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of our pairs list is: 47242\n",
      "There are 3 keys for each pair: \n",
      "['unaligned', 'aligned', 'name']\n",
      "Unaligned has these keys:\n",
      "['pcc', 'ssim', 'prmi', 'nmi']\n",
      "Aligned has these keys:\n",
      "['shift', 'nmi', 'ssim', 'prmi', 'pcc']\n",
      "pcc has these keys:\n",
      "['freq_c', 'global', 'time_c', 'spatial']\n"
     ]
    }
   ],
   "source": [
    "engine = Correlator() \n",
    "\n",
    "for f in folders:\n",
    "    pairs_data.extend(engine._score_multi('./fits_data/' + f))\n",
    "\n",
    "with open('pairs.npy', 'wb') as outfile:\n",
    "    pickle.dump(pairs_data, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "pair_info = np.load('pairs.npy')\n",
    "\n",
    "print(\"The length of our pairs list is: \"+ str(len(pair_info)))\n",
    "print(\"There are \" + str(len(pair_info[0].keys())) + \" keys for each pair: \")\n",
    "print(pair_info[0].keys())\n",
    "print(\"Unaligned has these keys:\")\n",
    "print(pair_info[0].get('unaligned').keys())\n",
    "print(\"Aligned has these keys:\")\n",
    "print(pair_info[0].get('aligned').keys())\n",
    "print(\"pcc has these keys:\")\n",
    "print(pair_info[0].get('aligned').get('pcc').keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# header.npy: Loading in Header Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ON_header_FCNTR = []\n",
    "OFF_header_FCNTR = []\n",
    "ON_header_DEC = []\n",
    "OFF_header_DEC = []\n",
    "ON_header_RA = []\n",
    "OFF_header_RA = []\n",
    "ON_header_MJD = []\n",
    "OFF_header_MJD = []\n",
    "\n",
    "for p in range(len(pairs)):   \n",
    "    ON_header_FCNTR.append(fits.open(pairs[p][0])[0].header['FCNTR'])\n",
    "    ON_header_DEC.append(fits.open(pairs[p][0])[0].header['DEC'])\n",
    "    ON_header_RA.append(fits.open(pairs[p][0])[0].header['RA'])\n",
    "    ON_header_MJD.append(fits.open(pairs[p][0])[0].header['MJD'])\n",
    "\n",
    "    OFF_header_FCNTR.append(fits.open(pairs[p][1])[0].header['FCNTR'])\n",
    "    OFF_header_DEC.append(fits.open(pairs[p][1])[0].header['DEC'])\n",
    "    OFF_header_RA.append(fits.open(pairs[p][1])[0].header['RA'])\n",
    "    OFF_header_MJD.append(fits.open(pairs[p][1])[0].header['MJD'])\n",
    "\n",
    "ON_header = zip(ON_header_FCNTR, ON_header_DEC, ON_header_RA, ON_header_MJD)\n",
    "OFF_header = zip(OFF_header_FCNTR, OFF_header_DEC, OFF_header_RA, OFF_header_MJD)\n",
    "\n",
    "with open('header.npy', 'wb') as outfile:\n",
    "    pickle.dump(zip(ON_header, OFF_header), outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corr_dataframe.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = np.load('corr_pair.pkl')\n",
    "header = np.load('header.npy')\n",
    "\n",
    "header_LOC_ON_X = [y[1] for y in [x[0] for x in header]]\n",
    "header_LOC_ON_Y = [y[2] for y in [x[0] for x in header]]\n",
    "header_MJD_ON = [y[3] for y in [x[0] for x in header]]\n",
    "\n",
    "header_LOC_OFF_X = [y[1] for y in [x[1] for x in header]]\n",
    "header_LOC_OFF_Y = [y[2] for y in [x[1] for x in header]]\n",
    "header_MJD_OFF = [y[3] for y in [x[1] for x in header]]\n",
    "\n",
    "\n",
    "df['ON_targ_X'] = pd.Series(header_LOC_ON_X, index=df.index)\n",
    "df['ON_targ_Y'] = pd.Series(header_LOC_ON_Y, index=df.index)\n",
    "\n",
    "df['OFF_targ_X'] = pd.Series(header_LOC_OFF_X, index=df.index)\n",
    "df['OFF_targ_Y'] = pd.Series(header_LOC_OFF_Y, index=df.index)\n",
    "\n",
    "with open('corr_dataframe.pkl', 'wb') as outfile:\n",
    "    pickle.dump(df, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG: Unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py27/lib/python2.7/site-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "for p in range(len(pairs)):\n",
    "    pair = []\n",
    "    fd1, hog_image = hog(skimage.color.rgb2gray(open_fits(str(pairs[p][0]))), orientations=72, pixels_per_cell=(64, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    fd2, hog_image = hog(skimage.color.rgb2gray(open_fits(str(pairs[p][1]))), orientations=72, pixels_per_cell=(64, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    hog_pairs.append(list((fd1, fd2)))\n",
    "\n",
    "with open('hog_image_64by16_72_wb.npy', 'wb') as outfile:\n",
    "    pickle.dump(hog_pairs, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG: Log Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py27/lib/python2.7/site-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "for p in range(len(pairs)):\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    fits_file = skimage.color.rgb2gray(open_fits(str(pairs[p][0])))\n",
    "    const = 10 * np.log10(fits_file)\n",
    "    max_val = np.amax(const)\n",
    "    fits_log = const/max_val\n",
    "    \n",
    "    fd1, hog_image = hog(fits_log, orientations=72, pixels_per_cell=(64, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    \n",
    "    fits_file_OFF = skimage.color.rgb2gray(open_fits(str(pairs[p][1])))\n",
    "    const_OFF = 10 * np.log10(fits_file)\n",
    "    max_val_OFF = np.amax(const)\n",
    "    fits_log_OFF = const/max_val\n",
    "    fd2, hog_image = hog(fits_log_OFF, orientations=72, pixels_per_cell=(64, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    \n",
    "    hog_pairs.append(list((fd1, fd2)))\n",
    "\n",
    "with open('hog_image_64by16_72_log.npy', 'wb') as outfile:\n",
    "    pickle.dump(hog_pairs, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = np.load('hog_image_64by16_72_log.npy')\n",
    "\n",
    "hog_fd(inp)\n",
    "\n",
    "with open('hog_image_64by16_72_log_p.npy', 'wb') as outfile:\n",
    "    pickle.dump(zip(result_array_ON, result_array_OFF), outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    fits_file = open_fits(str(pairs[x][0]))\n",
    "    const = 10 * np.log10(fits_file)\n",
    "    max_val = np.amax(const)\n",
    "    fits_log = (const/max_val)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(12, 3),\n",
    "                                    sharex=True, sharey=True)\n",
    "\n",
    "    # vmax = max([np.max(fits_file), np.max(fits_log)])\n",
    "    # vmin = min([np.min(fits_file), np.min(fits_log)])\n",
    "    vmax = 1\n",
    "    vmin = 0\n",
    "    \n",
    "    ax1.imshow(fits_file, cmap=plt.cm.gray, aspect = 20)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "    ax1.set_title('normal image', fontsize=20)\n",
    "\n",
    "    ax2.imshow(const, cmap=plt.cm.gray, aspect = 20)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_adjustable('box-forced')\n",
    "    ax2.set_title('log', fontsize=20)\n",
    "\n",
    "    ax3.imshow(fits_log, cmap=plt.cm.gray, aspect = 20)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_adjustable('box-forced')\n",
    "    ax3.set_title('log norm', fontsize=20)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG: binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAETCAYAAADTbHYBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFBhJREFUeJzt3W2MZNdd5/HvzzYOwhsMtoVHmcVjHGNZzmZiRXiwIgsq\nAe2MeYEjg4RtZIQhwlIwyiuweRG5d5VVYml3FSETRwPGKC+yExEjZRKw4gBukCFGg/ADCR78lPVT\nkkFJDEuC2B2c/77ompmaSnfXnepbD336+5FKqtv3dNW/rrp/ffrcc89NVSFJatNZiy5AkjQ7hrwk\nNcyQl6SGGfKS1DBDXpIaZshLUsPmHvJJ7k9yLMlTHdr+zySPJ/nbJP+Q5BvzqFGSWpF5z5NPch3w\nTeBjVbX3DL7vDuDqqnrPzIqTpMbMvSdfVY8Cr41+LcllSR5KciTJnye5Yp1vvRn4X3MpUpIacc6i\nCxg6CNxeVc8n2QfcB/zEiZ1JLgEuBf5sMeVJ0va08JBPch7wDuAPkmT45e8aa3YT8MlyDQZJOiML\nD3nWhoxeq6q3b9LmJuC9c6pHkpoxcUy+62yYJNckOZ7kxg7vm+GDqvoX4EtJfnbktfaOPL8S+L6q\neqzD60qSRnQ58foAsH+zBknOAj4EfHbSiyX5OPBXwBVJXkpyG/DzwC8neSLJF4CfHvmWnwMOdahT\nkjSm0xTKJHuAT2805THJ+4D/B1wDfKaq/rDXKiVJU9nyFMokbwLeXVX3MRyCkSQthz7myX8YuHNk\n26CXpCXRx+yaHwEODac/XgRcn+R4VR0eb5jEKZCSNIWqmqoD3bUnf3I2zDpvfNnw8UPAJ4H3rhfw\nI+19VHH33XcvvIZleXgsPBYei80fWzGxJz+cDTMALkzyEnA3cO5aXtfB8QzfUjWSpF5NDPmquqXr\ni1XVL22tHElSn1xPfkEGg8GiS1gaHotTPBaneCz6MdelhpPUPN9PklqQhJrxiVdJ0jZkyEtSwwx5\nSWqYIS9JDTPkJalhhrwkNcyQl6SGGfKS1DBDXpIaZshL2hZ27bqUJCTh7LPPO/l8165LF13aUjPk\nJS2l0VBPwrFjL7K20G3x7W//68nnx4599bR2hv7pDHlJS2M02EdDffNVzP/vae1GQ9/Ad4EySQu2\na9elw0A/4URGhNPDPRvs27xdC5njAmWStq3Te+x9e8OO79Xbk5e0UGu3hz7zHvo07bZr/tiTl7Rt\njJ9QnZ837MgTtPbkJc3V6T136KOHPm277ZJH9uQl6YztjPF6Q17SzI0O0SyPU1MvT5/d0xaHayTN\n3MYnV8e3F9dumbPJ4RpJ2pJ2h24mhnyS+5McS/LUBvtvSfLk8PFokrf2X6ak7WY5h2g20u7QTZee\n/APA/k32vwD8WFW9DfgA8Dt9FCZpe5vtRU6z1NZUy3MmNaiqR5Ps2WT/YyObjwG7+yhMkhbjRK9+\nzbFj2+E/kY31PSb/HuChnl9TkjSliT35rpK8E7gNuG6zdisrKyefDwYDBoNBXyVIWrDvXGysBW84\neV7h4ov38NWv/u+Zv+Pq6iqrq6u9vFanKZTD4ZpPV9XeDfbvBR4EDlTV85u8jlMopYZ1W4dmOadQ\nLvNUy3lMoczwsd6bX8JawN+6WcBLkuZv4nBNko8DA+DCJC8BdwPnAlVVB4H3AxcAH8nan/HjVbVv\ndiVLkrryildJvWl/uOa7WZt9M7/xefCKV0kLsrhlgxdl+100ZU9e0tSmWzZ4GXvoy30TEnvykjR3\n22O9m97myUvSznLqythlvirWnrwkNcyQl3RGttfqkjLkJZ2R7bu65Cwt78qVjslL0pYt78qV9uQl\nqWGGvCQ1zJCXNJEnW8/U8syh94pXSRN1W5Nms33LcYXq7NrNdnlir3iVJK3LkJekhhnyktQwQ16S\nZmqxJ2ENeUnfYeetEz9Li12D3tk1kr7DdOvEd223DLNhFveZpslAZ9dIktZlyEvS3Mx/ITMXKJOk\nuZn/Qmb25CUBLl3QKk+8SgI2W7pgfNsTr32+dpdMnOmJ1yT3JzmW5KlN2vxWkmeTPJHk6mkKkST1\nr8twzQPA/o12JrkeeHNV/TBwO/DRnmqTpMbN/kKpiSFfVY8Cr23S5AbgY8O2fw2cn+TifsqTpJbN\n/kKpPk687gZeHtl+dfg1SdKCzX0K5crKysnng8GAwWAw7xIkDe3adelCLrXX5lZXV1ldXe3ltTrN\nrkmyB/h0Ve1dZ99HgUeq6hPD7aPAj1fVsXXaOrtGWiL93gyka7tlmA2znJ9po3ycx7IGGT7Wcxj4\nhWEh1wL/tF7AS5I2M5uTsBOHa5J8HBgAFyZ5CbgbOBeoqjpYVX+c5KeSPAd8C7itt+okacc4dTVs\nn1fCejGUtIM5XLO8n2k0K12FUpK0LkNe2kG8GcjO4yqU0g6yNl1yfLhALbMnL0kNM+QlqWGGvCQ1\nzJCXpIYZ8lLjvOPTzmbIS407NaPGCxF3IkNekhpmyEtSwwx5SWqYIS9JDTPkpQY5o0YnGPJSg5xR\noxMMeUlqmCEvSQ0z5CWpYYa8JDXMkJca4B2ftBHvDCU1wDs+aSP25CWpYYa8JDWsU8gnOZDkaJJn\nkty5zv7vTXI4yRNJ/i7JL/ZeqSTpjE0M+SRnAfcC+4G3ADcnuXKs2a8CX6yqq4F3Av8jieP90gy5\ndIG66NKT3wc8W1UvVtVx4BBww1ibAt44fP5G4OtV9e/9lSlpnEsXqIsuIb8beHlk+5Xh10bdC1yV\n5MvAk8D7+ilPkrQVfQ2p7Acer6p3JXkz8Lkke6vqm+MNV1ZWTj4fDAYMBoOeSpCkdoxm5VakavN/\n9ZJcC6xU1YHh9l1AVdU9I20+A3ywqv5yuP2nwJ1V9Tdjr1WT3k9SN2tj8Sd+n0afj28vW7tlqGH5\nP9NoViahqqY6+dJluOYIcHmSPUnOBW4CDo+1eRH4yWExFwNXAC9MU5AkqT8TQ76qXgfuAB4Gvggc\nqqqnk9ye5FeGzT4AvCPJU8DngN+oqm/Mqmhpp3JGjc7UxOGaXt/M4RppSzYeolmGIYuu7ZahhuX/\nTPMcrpEkbVOGvCQ1zJCXpIYZ8tKS82SrtsKQl5acyxdoKwx5SWqYIS9JDTPkJalhhrwkNcyQl5bM\n6GwaZ9Roq7x7k7RkTs2mOcGg1/TsyUtSwwx5SWqYIS9JDTPkpSXg0gWaFUNeWgIuXaBZMeQlqWGG\nvCQ1zJCXpIYZ8pLUMENeWhBn1GgeDHlpQZxRo3kw5CWpYZ1CPsmBJEeTPJPkzg3aDJI8nuQLSR7p\nt0xJ0jQmrkKZ5CzgXuAngC8DR5J8qqqOjrQ5H/ht4D9X1atJLppVwZKk7rr05PcBz1bVi1V1HDgE\n3DDW5hbgwap6FaCqvtZvmZKkaXQJ+d3AyyPbrwy/NuoK4IIkjyQ5kuTWvgqUJE2vr5uGnAO8HXgX\ncB7w+SSfr6rnxhuurKycfD4YDBgMBj2VIC23XbsuHc6okSYbzcqtSNXm07eSXAusVNWB4fZdQFXV\nPSNt7gS+u6r+y3D7d4GHqurBsdeqSe8ntWptPvz4HZ9qwvOu7Wb52ju51sV9ptGsTEJVTXVBRZfh\nmiPA5Un2JDkXuAk4PNbmU8B1Sc5O8j3AjwJPT1OQJKk/E4drqur1JHcAD7P2R+H+qno6ye1ru+tg\nVR1N8lngKeB14GBV/f1MK5ckTTRxuKbXN3O4RjuYwzXLVMPyf6Z5DtdIkrYpQ16aIRch06IZ8tIM\nuQiZFs2Ql6SGGfKS1DBDXpIaZshLUsMMealnzqjRMjHkpZ45o0bLxJCXpIYZ8pLUMENekhpmyEtS\nwwx5SWqYIS9t0eiUSadNatn0dY9Xacc6NWXyBINey8OevCQ1zJCXpIYZ8pLUMENekhpmyEtSwwx5\naQquNKntwpCXpuBKk9ouOoV8kgNJjiZ5Jsmdm7S7JsnxJDf2V6IkaVoTQz7JWcC9wH7gLcDNSa7c\noN2HgM/2XaQkaTpdevL7gGer6sWqOg4cAm5Yp92vAZ8E/rHH+iRJW9Al5HcDL49svzL82klJ3gS8\nu6ruw2u6JWlp9LV2zYeB0bH6DYN+ZWXl5PPBYMBgMOipBElqx2hWbkWqNp8dkORaYKWqDgy37wKq\nqu4ZafPCiafARcC3gF+pqsNjr1WT3k9aVrt2XTqcVXPCiZ/l8J0LlK23b5btlqGGFmtd3Gcazcok\nVNVUoyRdevJHgMuT7AG+AtwE3DzaoKouGynmAeDT4wEvbXenrzbpqKS2h4khX1WvJ7kDeJi1Mfz7\nq+rpJLev7a6D498ygzolSVOYOFzT65s5XKNtbO3q1p0zXGCti/1MfQ3XeMWrJDXMkJekhhnyktQw\nQ17agDfoVgu8kbe0AW/QrRbYk5ekhhnyktQwQ16SGmbIS1LDDHlphPduVWsMeWmE925Vawx5SWqY\nIS9JDTPkJalhhrwkNcyQ147njBq1zJDXjueMGrXMkJekhhnyktQwQ16SGmbIS1LDDHntON7xSTuJ\nd4bSjuMdn7STdOrJJzmQ5GiSZ5Lcuc7+W5I8OXw8muSt/ZcqSTpTE0M+yVnAvcB+4C3AzUmuHGv2\nAvBjVfU24APA7/RdqCTpzHXpye8Dnq2qF6vqOHAIuGG0QVU9VlX/PNx8DNjdb5mSpGl0CfndwMsj\n26+weYi/B3hoK0VJfXPpAu1UvZ54TfJO4Dbguo3arKysnHw+GAwYDAZ9liCt6/STrQa9lt9oVm5F\nqjZfryPJtcBKVR0Ybt8FVFXdM9ZuL/AgcKCqnt/gtWrS+0mzsNaDHw358dk16+3bTu2WoYYWa13c\nZxrNyiRU1VS9ky7DNUeAy5PsSXIucBNweLRBkktYC/hbNwp4SdL8TRyuqarXk9wBPMzaH4X7q+rp\nJLev7a6DwPuBC4CPZK3LdLyq9s2ycEnSZBOHa3p9M4drNEe7dl06HIs/YZn+ve+73TLU0GKti/tM\n8xyukbYl14mXDHlJapohL0kNM+QlqWGGvJrila3S6Qx5NcWTrdLpDHlJapghr23NuzxJm/POUNrW\nvMuTtDl78pLUMENekhpmyGvbcZqk1J0hr23HaZJSd4a8JDXMkNe24BCNNB1DXtuCQzTSdAx5LSUv\ncpL64cVQWkpe5CT1w568JDXMkNfS8OSq1D9DXkvDk6tS/wx5LZS9d2m2OoV8kgNJjiZ5JsmdG7T5\nrSTPJnkiydX9lqlWjM+asfcuzdbEkE9yFnAvsB94C3BzkivH2lwPvLmqfhi4HfjoDGptyurq6qJL\nmJvRYD891E8E++rCals+q4suQI3p0pPfBzxbVS9W1XHgEHDDWJsbgI8BVNVfA+cnubjXShvTeshv\nHOzrWZ1fYUtvddEFqDFdQn438PLI9ivDr23W5tV12qgBo+F99tnnrfvcYRhpecz9xOuJELjqqv9E\nlQEwrY3CdrPwnabdZuH97W//67rPDXZpeWRS0Ca5FlipqgPD7buAqqp7Rtp8FHikqj4x3D4K/HhV\nHRt7LX/7JWkKVTXVFLQuyxocAS5Psgf4CnATcPNYm8PArwKfGP5R+KfxgN9KkZKk6UwM+ap6Pckd\nwMOsDe/cX1VPJ7l9bXcdrKo/TvJTSZ4DvgXcNtuyJUldTByukSRtXzM58erFU6dMOhZJbkny5PDx\naJK3LqLOeejyczFsd02S40lunGd989Txd2SQ5PEkX0jyyLxrnJcOvyPfm+TwMCv+LskvLqDMmUty\nf5JjSZ7apM2Z52ZV9fpg7Q/Hc8Ae4LuAJ4Arx9pcD/zR8PmPAo/1XccyPDoei2uB84fPD+zkYzHS\n7k+BzwA3LrruBf5cnA98Edg93L5o0XUv8Fj8JvDBE8cB+DpwzqJrn8GxuA64Gnhqg/1T5eYsevJe\nPHXKxGNRVY9V1T8PNx+j3esLuvxcAPwa8EngH+dZ3Jx1ORa3AA9W1asAVfW1Odc4L12ORQFvHD5/\nI/D1qvr3OdY4F1X1KPDaJk2mys1ZhLwXT53S5ViMeg/w0EwrWpyJxyLJm4B3V9V9tH2XkC4/F1cA\nFyR5JMmRJLfOrbr56nIs7gWuSvJl4EngfXOqbdlMlZveGWpJJHkna7OSrlt0LQv0YWB0TLbloJ/k\nHODtwLuA84DPJ/l8VT232LIWYj/weFW9K8mbgc8l2VtV31x0YdvBLEL+VeCSke3/OPzaeJsfnNCm\nBV2OBUn2AgeBA1W12b9r21mXY/EjwKGsrTt8EXB9kuNVdXhONc5Ll2PxCvC1qvo34N+S/AXwNtbG\nr1vS5VjcBnwQoKqeT/Il4Ergb+ZS4fKYKjdnMVxz8uKpJOeydvHU+C/pYeAX4OQVtetePNWAicci\nySXAg8CtVfX8Amqcl4nHoqouGz5+iLVx+fc2GPDQ7XfkU8B1Sc5O8j2snWh7es51zkOXY/Ei8JMA\nwzHoK4AX5lrl/ISN/4OdKjd778mXF0+d1OVYAO8HLgA+MuzBHq+qfYurejY6HovTvmXuRc5Jx9+R\no0k+CzwFvA4crKq/X2DZM9Hx5+IDwO+PTC38jar6xoJKnpkkHwcGwIVJXgLuBs5li7npxVCS1DBv\n/ydJDTPkJalhhrwkNcyQl6SGGfKSNCNdFh0baXtJkj8ZLlb4Z8MrwLfMkJek2XmAtSt2u/jvwO9X\n1duA/wp8qI8CDHlJmpH1Fh1LclmSh4ZrEv15kiuGu64CHhl+3yrrL+B3xgx5SZqvg8AdVXUN8OvA\nfcOvPwHcCDC8l8J/SPL9W30zFyiTpDlJch7wDuAPhle4w9o6+rAW+PcOb4ryF6ytS/P6Vt/TkJek\n+TkLeK2q3j6+o6q+AvwMnPxj8DNV9X/6eENJ0uycXHSsqv4F+FKSnz25c20VWpJcONK7/03g9/p4\nc0NekmZkuOjYXwFXJHkpyW3AzwO/PLxP6xeAnx42HwD/kOQo8APAf+ulBhcok6R22ZOXpIYZ8pLU\nMENekhpmyEtSwwx5SWqYIS9JDTPkJalhhrwkNez/A1c6enXI9fV5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11326dcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking for a threshold to binarize\n",
    "flatten_pairs = [val for sublist in pairs for val in sublist]\n",
    "\n",
    "data = np.array([fio.read(file) for file in flatten_pairs])\n",
    "all_data = np.sort(data.flatten())\n",
    "\n",
    "plt.hist(all_data[:500401292], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = all_data[int(500401292 * 0.8)]\n",
    "data = fio.read(file)\n",
    "data_flat = np.sort(data.flatten())\n",
    "threshold = max([THRESHOLD, data_flat[int(.8*len(data_flat))]])\n",
    "binary_data = np.copy(data)\n",
    "binary_data[binary_data < threshold] = 0\n",
    "binary_data[binary_data >= threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hog_pairs = []\n",
    "\n",
    "for x in pairs:\n",
    "    data = fio.read(x[0])\n",
    "    data_flat = np.sort(data.flatten())\n",
    "    threshold = max([THRESHOLD, data_flat[int(.8*len(data_flat))]])\n",
    "    binary_data = np.copy(data)\n",
    "    binary_data[binary_data < threshold] = 0\n",
    "    binary_data[binary_data >= threshold] = 1\n",
    "    fd1, hog_image_bin = hog(binary_data, orientations=72, pixels_per_cell=(64, 16), cells_per_block=(1, 1), visualise=True)    \n",
    "\n",
    "    data = fio.read(x[1])\n",
    "    data_flat = np.sort(data.flatten())\n",
    "    threshold = max([THRESHOLD, data_flat[int(.8*len(data_flat))]])\n",
    "    binary_data = np.copy(data)\n",
    "    binary_data[binary_data < threshold] = 0\n",
    "    binary_data[binary_data >= threshold] = 1\n",
    "    fd2, hog_image_bin = hog(binary_data, orientations=72, pixels_per_cell=(64, 16), cells_per_block=(1, 1), visualise=True)    \n",
    "\n",
    "    hog_pairs.append(list((fd1, fd2)))\n",
    "    \n",
    "with open('binary_hog.npy', 'wb') as outfile:\n",
    "    pickle.dump(hog_pairs, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "inp = np.load('binary_hog.npy')\n",
    "result_array_ON = np.empty((0, 21))\n",
    "result_array_OFF = np.empty((0, 21))\n",
    "\n",
    "hog_fd(inp)\n",
    "\n",
    "with open('binary_hog_p.npy', 'wb') as outfile:\n",
    "    pickle.dump(zip(result_array_ON, result_array_OFF), outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing all HOG versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in flatten_pairs[6:8]:\n",
    "    f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(20,15))\n",
    "    data = skimage.color.rgb2gray(fio.read(file))\n",
    "    data_flat = np.sort(data.flatten())\n",
    "    threshold = max([THRESHOLD, data_flat[int(.8*len(data_flat))]])\n",
    "    \n",
    "    const = 10 * np.log10(data)\n",
    "    max_val = np.amax(const)\n",
    "    fits_log = const/max_val\n",
    "    \n",
    "    _, log_image = hog(fits_log, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    \n",
    "    \n",
    "    binary_data = np.copy(data)\n",
    "    binary_data[binary_data < threshold] = 0\n",
    "    binary_data[binary_data >= threshold] = 1\n",
    "    _, hog_image = hog(data, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True)\n",
    "    _, hog_image_bin = hog(binary_data, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True)    \n",
    "    ax1.imshow(data, aspect=\"auto\")\n",
    "    ax2.imshow(hog_image, aspect=\"auto\")\n",
    "    ax3.imshow(log_image, aspect=\"auto\")\n",
    "    ax4.imshow(hog_image_bin, aspect=\"auto\")\n",
    "    \n",
    "    hfont = {'fontname':'Baskerville'}\n",
    "\n",
    "    ax1.set_title(\"Original plot\", fontsize='20', **hfont)\n",
    "    ax2.set_title(\"HOG plot\", fontsize='20', **hfont)\n",
    "    ax3.set_title(\"Log normalized HOG plot\", fontsize='20', **hfont)\n",
    "    ax4.set_title(\"Binarized HOG plot\", fontsize='20', **hfont)\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    f.subplots_adjust(hspace=.5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod_corr_dataframe.pkl: + intensity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header_MJD_ON = [y[3] for y in [x[0] for x in header]]\n",
    "header_MJD_OFF = [y[3] for y in [x[1] for x in header]]\n",
    "\n",
    "intensity = []\n",
    "for x in pairs:\n",
    "    rgbImage = fitsio.read(x[0]);\n",
    "    grayImage = rgb2gray(rgbImage);\n",
    "    ON_gray = sum(sum(grayImage));\n",
    "    \n",
    "    rgbImage = fitsio.read(x[1]);\n",
    "    grayImage = rgb2gray(rgbImage);\n",
    "    OFF_gray = sum(sum(grayImage));\n",
    "    \n",
    "    intensity.append(ON_gray - OFF_gray)\n",
    "\n",
    "edge = np.load('hog_image_64by16_72_wb_ed_p.npy')\n",
    "\n",
    "for x in range(len(pairs)):\n",
    "    for y in range(len(edge[0])):\n",
    "        r = edge[x][y]\n",
    "        r[r <= 1] = 0\n",
    "\n",
    "        r[r > 1] = 1\n",
    "\n",
    "ON_unique = []\n",
    "OFF_unique = []\n",
    "\n",
    "for x in edge:\n",
    "    diff = list(x[0] - x[1])\n",
    "    ON_unique.append(diff.count(1))\n",
    "    OFF_unique.append(diff.count(-1))\n",
    "\n",
    "modified_corr = pd.read_pickle(\"corr_dataframe.pkl\")\n",
    "header = np.load('header.npy')\n",
    "freq = [lip[0][0] for lip in header]\n",
    "star = map(lambda x: x[:8], modified_corr.index.tolist())\n",
    "\n",
    "del modified_corr['a_shift']\n",
    "del modified_corr['u_freq_c']\n",
    "del modified_corr['u_global']\n",
    "del modified_corr['u_nmi']\n",
    "del modified_corr['u_prmi']\n",
    "del modified_corr['u_spatial']\n",
    "del modified_corr['u_time_c']\n",
    "del modified_corr['u_ssim']\n",
    "\n",
    "modified_corr['a_freq_c'] = modified_corr['a_freq_c'].abs()\n",
    "modified_corr['a_global'] = modified_corr['a_global'].abs()\n",
    "modified_corr['a_nmi'] = modified_corr['a_nmi'].abs()\n",
    "modified_corr['a_prmi'] = modified_corr['a_prmi'].abs()\n",
    "modified_corr['a_spatial'] = modified_corr['a_spatial'].abs()\n",
    "modified_corr['a_ssim'] = modified_corr['a_ssim'].abs()\n",
    "modified_corr['a_time_c'] = modified_corr['a_time_c'].abs()\n",
    "\n",
    "modified_corr['ON_mjd'] = pd.Series(header_MJD_ON, index=modified_corr.index)\n",
    "modified_corr['OFF_mjd'] = pd.Series(header_MJD_OFF, index=modified_corr.index)\n",
    "modified_corr['intensity'] = pd.Series(intensity, index=modified_corr.index)\n",
    "\n",
    "modified_corr['ON_unique'] = pd.Series(ON_unique, index=modified_corr.index)\n",
    "modified_corr['OFF_unique'] = pd.Series(OFF_unique, index=modified_corr.index)\n",
    "\n",
    "modified_corr['freq'] = pd.Series(freq, index=df.index)\n",
    "modified_corr['star'] = pd.Series(star, index=modified_corr.index)\n",
    "\n",
    "total = sum(modified_corr['intensity'])\n",
    "old_min = min(modified_corr['intensity'])\n",
    "old_range = max(modified_corr['intensity'])\n",
    "output = map(lambda x, r=float(old_range - old_min): ((x - old_min) / r)*10 - 1, modified_corr['intensity'].tolist())\n",
    "rounded = [ round(elem, 2) for elem in output ] \n",
    "modified_corr['mod_inten'] = pd.Series(rounded, index=modified_corr.index)\n",
    "\n",
    "with open('mod_corr_dataframe.pkl', 'wb') as outfile:\n",
    "    pickle.dump(modified_corr, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('comb_corr_hog.pkl')\n",
    "df2 = pd.read_pickle('mod_corr_dataframe.pkl')\n",
    "\n",
    "bigdata = pd.concat([df, df2], axis=1)\n",
    "\n",
    "with open('dataframe.pkl', 'wb') as outfile:\n",
    "    pickle.dump(bigdata, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
